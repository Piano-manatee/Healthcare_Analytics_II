{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true,
    "papermill": {
     "duration": 0.005095,
     "end_time": "2021-01-19T21:51:53.821181",
     "exception": false,
     "start_time": "2021-01-19T21:51:53.816086",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Problem Statement\n",
    "Recent Covid-19 Pandemic has raised alarms over one of the most overlooked area to focus: Healthcare Management. While healthcare management has various use cases for using data science, patient length of stay is one critical parameter to observe and predict if one wants to improve the efficiency of the healthcare management in a hospital. \n",
    "This parameter helps hospitals to identify patients of high LOS risk (patients who will stay longer) at the time of admission. Once identified, patients with high LOS risk can have their treatment plan optimized to miminize LOS and lower the chance of staff/visitor infection. Also, prior knowledge of LOS can aid in logistics such as room and bed allocation planning.\n",
    "Suppose you have been hired as Data Scientist of HealthMan – a not for profit organization dedicated to manage the functioning of Hospitals in a professional and optimal manner.\n",
    "The task is to accurately predict the Length of Stay for each patient on case by case basis so that the Hospitals can use this information for optimal resource allocation and better functioning. The length of stay is divided into 11 different classes ranging from 0-10 days to more than 100 days.\n",
    "\n",
    "Data Description\n",
    "traindata.csv – File containing features related to patient, hospital and Length of stay on case basis traindata_dictonary.csv – File containing the information of the features in train file\n",
    "\n",
    "Test Set\n",
    "testdata.csv – File containing features related to patient, hospital. Need to predict the Length of stay for each caseid\n",
    "\n",
    "Sample Submission:\n",
    "\n",
    "case_id: Unique id for each case\n",
    "\n",
    "Stay: Length of stay for the patient w.r.t each case id in test data\n",
    "\n",
    "Evaluation Metric\n",
    "The evaluation metric for this hackathon is 100*Accuracy Score.\n",
    "\n",
    "Acknowledgements\n",
    "More details can be found on Analytics Vidhya website who conducted the hackathon.\n",
    "https://datahack.analyticsvidhya.com/contest/janatahack-healthcare-analytics-ii/#ProblemStatement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-01-19T21:51:53.843993Z",
     "iopub.status.busy": "2021-01-19T21:51:53.838956Z",
     "iopub.status.idle": "2021-01-19T21:52:15.796633Z",
     "shell.execute_reply": "2021-01-19T21:52:15.797479Z"
    },
    "papermill": {
     "duration": 21.97209,
     "end_time": "2021-01-19T21:52:15.797691",
     "exception": false,
     "start_time": "2021-01-19T21:51:53.825601",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/av-healthcare-analytics-ii/healthcare/sample_sub.csv\n",
      "/kaggle/input/av-healthcare-analytics-ii/healthcare/train_data_dictionary.csv\n",
      "/kaggle/input/av-healthcare-analytics-ii/healthcare/train_data.csv\n",
      "/kaggle/input/av-healthcare-analytics-ii/healthcare/test_data.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "sample_sub_filepath = \"../input/av-healthcare-analytics-ii/healthcare/sample_sub.csv\"\n",
    "train_data_dictionary_filepath = \"../input/av-healthcare-analytics-ii/healthcare/train_data_dictionary.csv\"\n",
    "train_data_filepath = \"../input/av-healthcare-analytics-ii/healthcare/train_data.csv\"\n",
    "test_data_filepath = \"../input/av-healthcare-analytics-ii/healthcare/test_data.csv\"\n",
    "\n",
    "sample_sub = pd.read_csv(sample_sub_filepath)\n",
    "train_data_dictionary = pd.read_csv(train_data_dictionary_filepath)\n",
    "train_data = pd.read_csv(train_data_filepath)\n",
    "test_data = pd.read_csv(test_data_filepath)\n",
    "\n",
    "# Determine objective in training data\n",
    "train_y = train_data.Stay\n",
    "\n",
    "# Remove unnecessary columns in train & test\n",
    "features = [\"Type of Admission\",\"Severity of Illness\",\"Age\"]\n",
    "train_X = train_data[features]\n",
    "val_X = test_data[features]\n",
    "\n",
    "# Label encoding (X)\n",
    "    # Get relevant columns\n",
    "label_encoder_features = [\"Severity of Illness\", \"Age\"]\n",
    "    # Make copy to preserve original data\n",
    "label_train_X = train_X.copy()\n",
    "label_val_X = val_X.copy()\n",
    "label_train_X = label_train_X[label_encoder_features]\n",
    "label_val_X = label_val_X[label_encoder_features]\n",
    "    # Label encode\n",
    "label_encoder=LabelEncoder()\n",
    "for col in label_encoder_features:\n",
    "    label_train_X[col]=label_encoder.fit_transform(label_train_X[col])\n",
    "    label_val_X[col]=label_encoder.transform(label_val_X[col])\n",
    "\n",
    "# Label encoding (y)\n",
    "label_train_y = train_y.copy()\n",
    "label_train_y = label_encoder.fit_transform(label_train_y)\n",
    "\n",
    "# One hot encoding (X)\n",
    "    # Get relevant columns\n",
    "one_hot_encoder_features = [\"Type of Admission\"]\n",
    "    # Make copy to preserve original data\n",
    "one_hot_train_X = train_X.copy()\n",
    "one_hot_val_X = val_X.copy()\n",
    "one_hot_train_X = one_hot_train_X[one_hot_encoder_features]\n",
    "one_hot_val_X = one_hot_val_X[one_hot_encoder_features]\n",
    "    # One hot encode \n",
    "one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "one_hot_train_X = pd.DataFrame(one_hot_encoder.fit_transform(one_hot_train_X))\n",
    "one_hot_val_X = pd.DataFrame(one_hot_encoder.transform(one_hot_val_X))\n",
    "    # Get back columns names \n",
    "one_hot_train_X.columns = one_hot_encoder.get_feature_names(one_hot_encoder_features)\n",
    "one_hot_val_X.columns = one_hot_encoder.get_feature_names(one_hot_encoder_features)\n",
    "\n",
    "# Concatenate label and one hot encoding\n",
    "concat_train_X = pd.concat([label_train_X,one_hot_train_X],axis=1)\n",
    "concat_val_X = pd.concat([label_val_X,one_hot_val_X],axis=1)\n",
    "\n",
    "# Make predictions using the XGB model\n",
    "XGB_model = XGBRegressor(n_estimators=500)\n",
    "XGB_model.fit(concat_train_X, label_train_y)\n",
    "\n",
    "# Undo label encoding for predictions\n",
    "predictions = XGB_model.predict(concat_val_X).round()\n",
    "predictions = list(label_encoder.inverse_transform(predictions.astype(int)))\n",
    "predictions = np.array(predictions)\n",
    "\n",
    "# Save predictions to file\n",
    "output = pd.DataFrame( {\"case_id\" : test_data[\"case_id\"],\n",
    "                        \"Stay\": predictions})\n",
    "output.to_csv('submission.csv', index=False)\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.004563,
     "end_time": "2021-01-19T21:52:15.810034",
     "exception": false,
     "start_time": "2021-01-19T21:52:15.805471",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 26.366844,
   "end_time": "2021-01-19T21:52:15.922908",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-01-19T21:51:49.556064",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
